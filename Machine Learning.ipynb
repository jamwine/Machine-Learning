{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI vs ML vs DL\n",
    "### Artificial Intelligence (AI)\n",
    "A branch of computer science dealing with the simulation of intelligent behavior in computers. In layman's term, **AI** is a decision making algorithm that mimics human mind by doing smart work intelligently to solve complex problems.\n",
    "\n",
    "![ai](imgs/ai.png)\n",
    "\n",
    "### Machine Learning (ML)\n",
    "\n",
    "**Machine Learning** is the field of study that gives computers the ability to learn without being explicitly programmed. It is the study and construction of programs that are not explicitly programmed, but learn patterns as they are exposed to more data over time. The goal is to self-learn from data on certain task to maximize the performance of machine using experience on this task.\n",
    "\n",
    "A computer program is said to learn from **experience E** with respect to some class of **tasks T** and **performance measure P**, if its performance at tasks in T, as measured by P, improves with experience E. Consider an example, playing checkers where:\n",
    "* **E** = the experience of playing many games of checkers\n",
    "* **T** = the task of playing checkers.\n",
    "* **P** = the probability that the program will win the next game.\n",
    "\n",
    "In **supervised learning**, we are given a data set and already know what our correct output should look like, having the idea that there is a relationship between the input and the output. Supervised learning problems are categorized into *regression* and *classification* problems.\n",
    "\n",
    "**Unsupervised learning** allows us to approach problems with little or no idea what our results should look like. We can derive structure from data where we don't necessarily know the effect of the variables. We can derive this structure by *clustering* the data based on relationships among the variables in the data. With unsupervised learning there is no feedback based on the prediction results.\n",
    "\n",
    "### Deep Learning (DL)\n",
    "Deep learning is a subset of machine learning in which multilayered neural networks learn from vast amounts of data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differentiating Artificial Learning from Machine Learning\n",
    "|AI|ML|\n",
    "|---|---|\n",
    "|The aim is to increase chance of success and not accuracy.|The aim is to increase accuracy, but it does not care about success.|\n",
    "|AI will go for finding the optimal solution.|ML will go for only solution for that whether it is optimal or not.|\n",
    "|AI leads to intelligence or wisdom.|ML leads to knowledge.|\n",
    "|AI manages more comprehensive issues of automating a system. This computerization should be possible by utilizing any field such as image processing, cognitive science, neural systems, machine learning etc.|Machine Learning (ML) manages influencing user’s machine to gain from the external environment. This external environment can be sensors, electronic segments, external storage gadgets and numerous other devices.|\n",
    "|AI manages the making of machines, frameworks and different gadgets savvy by enabling them to think and do errands as all people generally do.|What ML does, depends on the user input or a query requested by the client, the framework checks whether it is available in the knowledge base or not. If it is available, it will restore the outcome to the user related with that query, however if it isn’t stored initially, the machine will take in the user input and will enhance its knowledge base, to give a better value to the end user.|\n",
    "\n",
    "Examples of **Artificial Intelligence** include:\n",
    "* Voice assistants, such as Siri\n",
    "* Recommendation systems, such as Netflix\n",
    "* Self-driving cars\n",
    "* Drones that fly over fields and capture footage used to optimize crop yield\n",
    "* Google Search\n",
    "* Surfacing algorithms, such as those employed by Twitter and Facebook, that decide what content to show you in our feed\n",
    "\n",
    "Examples of **Machine Learning** include:\n",
    "* Predicting whether a given credit card transaction is fraudulent or not, given transaction details\n",
    "* Predicting whether an email is spam or not, given the email sender, subject, and body\n",
    "* Predicting the diagnosis of a particular piece of medical imaging\n",
    "* Predicting the present and future location of pedestrians, cars, and other stationary/moving objects in a video feed (such as those used by self-driving cars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning Workflow\n",
    "The machine learning workflow consists of:\n",
    "\n",
    "* **Problem statement**: What problem are we trying to solve?\n",
    "* **Data collection**: What data do you need to solve it?\n",
    "* **Data exploration and preprocessing**: How should we clean our data so our model can use it?\n",
    "* **Modeling**: Build a model to solve our problem?\n",
    "* **Validation**: Did we solve the problem?\n",
    "* **Decision Making and Deployment**: Communicate to stakeholders or put into production?\n",
    "\n",
    "### Common Terminologies of data for Machine Learning:\n",
    "* **target**: category or value we are trying to predict\n",
    "* **observation**: an example or single data point within the data (usually a point or row in dataset)\n",
    "* **features**: explanatory variables used for prediction for each observation (usually a column)\n",
    "* **label**: the value of the target for a single data point (output variable being predicted)\n",
    "* **algorithms**: computer programs that estimate models based on available data\n",
    "* **model**: hypothesized relationship between observations and data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming Data\n",
    "\n",
    "Models used in Machine Learning Workflows often make assumptions about the data. For example, linear regression model assumes a linear relationship between observations and target (outcome) variables. An example of a linear model relating (feature) variables $x_1$ and $x_2$ with target (label) variable $y$, is:\n",
    "\n",
    "$$y_\\beta(x)=\\beta_0+\\beta_1x_1+\\beta_2x_2$$\n",
    "\n",
    "where $\\beta=(\\beta_0,\\beta_1,\\beta_2)$ represents the model's parameter.\n",
    "\n",
    "Predictions from linear regression models assume **residuals are normally distributed**. However, the raw data (having features) and predicted data are often **skewed** (distorted away from the center), hence we TRANSFORM the data.\n",
    "\n",
    "**Log transformations** can be a useful way to find a linear relationship when the underlying raw data may not actually have a linear relationship. \n",
    "```python\n",
    "# Useful transformation functions\n",
    "from numpy import log,log1p\n",
    "from scipy.stats import boxcox\n",
    "```\n",
    "\n",
    "So the resulting algorithm will still be a linear regression since the outcome is still a linear combinations of the features $y_\\beta(x)=\\beta_0+\\beta_1log(x_1)$ (the features have been transformed). Now the linear regression involves a linear combination of our new features, one of our new features being the log(x) rather than just x.\n",
    "\n",
    "Similarly, we can estimate higher-order relationships by adding polynomial features to fit a linear model. \n",
    "$$y_\\beta(x)=\\beta_0+\\beta_1x+\\beta_2x^2$$\n",
    "\n",
    "Again, we're changing our features, but maintaining a linear model with features being transformed into squared and cubed.\n",
    "\n",
    "### Variable Selection\n",
    "This involves choosing the set of features to include in the model. Variables must often be transformed before they can be included in models. In addition to log and polynomial transformations, this can involve:\n",
    "* **Encoding**: converting non-numeric features to numeric features\n",
    "* **Scaling**: converting the scale of numeric data so they are comparable.\n",
    "\n",
    "The appropriate method of encoding and scaling depends on the type of feature. **Feature Encoding** is often applied to categorical features, two primary types are: \n",
    "* Nominal: categorical variables takes values in unordered categories (eg. red, blue, green, True, False)\n",
    "* Ordinal: categorical variables takes values in ordered categories (eg. low, medium, high)\n",
    "\n",
    "There are several common approaches to encoding variables:\n",
    "* **Binary Encoding**: converts variable to either 0 or 1 and is suitable for variables that takes two possible values (eg. True, False)\n",
    "* **One-hot Encoding**: converts variables that takes multiple values into binary (0,1) variables, one for each category. This creates several new variables.\n",
    "* **Ordinal Encoding**: involves converting ordered categories to numerical values, usually by creating one variable that takes integer equal to the number of categories (eg. 0,1,2,3,...).\n",
    "\n",
    "**Feature Scaling** involves adjusting a variable's scale. This allows comparison of variables with different scales, as different continuous (numeric) features often have different scales. Some common approaches to scaling features are:\n",
    "* **Standard Scaling**: converts features to standard normal variables (by subtracting the mean and dividing by the standard error)\n",
    "* **Min-max Scaling**: converts variables to continuous variables in the (0,1) interval by mapping minimum values to 0 and maximum values to 1. This type of scaling is sensitive to outliers. \n",
    "* **Robust Scaling**: is similar to min-max scaling, but instead maps the interquartile range to (0,1). This means the variable itself takes values outside of the (0,1) interval.\n",
    "\n",
    "```python\n",
    "# Common variable transformation\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "\n",
    "# Functions for encoding categorical variables\n",
    "from sklearn.preprocessing import LabelEncoder, LabelBinarizer, OneHotEncoder\n",
    "from pandas import get_dummies\n",
    "\n",
    "# Functions for encoding ordinal variables\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimation vs. Inference\n",
    "**Estimation** is the application of an algorithm. Estimate is just going to give us an estimate of a certain parameter, such as the mean from our sample data:\n",
    "$$\\bar{X}=\\sum_{i=1}^{N}\\frac{x_i}{n}$$\n",
    "\n",
    "**Inference** involves putting an accuracy on the estimate (eg. standard error of an average):\n",
    "$$[\\sum_{i=1}^{N}\\frac{(x_i-\\bar{x})^2}{(n-1)}]^{1/2}$$\n",
    "\n",
    "Estimation is only a part when we talk about statistical inference. When performing statistical inference, we're trying to understand the underlying distribution of the population, including our estimates of the mean, as well as other parameters such as the standard error of the underlying properties of the population that we're sampling from.\n",
    "\n",
    "### Parametric vs. Non-Parametric\n",
    "A **statistical inference** is about finding the underlying **Data Generating Process (DGP)** of our data, then the **statistical model** is going to be a set of the possible distributions or even regression that data can take. \n",
    "\n",
    "A **parametric model** is a particular type of statistical model that is constrained to a finite number of parameters and relies on some strict assumptions made about the distributions from which that data is pulled. For example, we have to predefine the number of coefficients according to the features or transformed features in a linear distribution, hence parametric as we are having the constraints that come along with it. Another example of a parametric model is the normal distribution, that depends on a set number of parameters, namely the mean and the standard deviation\n",
    "\n",
    "**Non-parametric models** means that our inference will not rely on as many assumptions, such as it will not have to rely on the data being pulled from a particular distribution, it'll be a distribution free inference. An example of a non-parametric inference is creating a distribution of the data (CDF or cumulative distribution function) using a histogram. CDF or the Cumulative Distribution Function defines the probability of where a certain value will fall according to the actual data, thus it is a distribution defined by the actual data that we've pulled from our sample. In this case, we wouldn't be specifying any of the parameters. \n",
    "\n",
    "Consider a business example, customer lifetime value, an estimate of the customer's value to the company over time. The data related to customer lifetime value will include the expected length of time as a customer, the type and amount of purchases that customer has made, etc. So to estimate lifetime value, we need to make assumptions about the data, namely, how long do we think customers' going to last as well as how much do we think they will spend over time? These assumptions can be parametric by assuming a specific distribution, whether that's linear over time or some type of decrease over time. Also, these assumptions can be non-parametric, where we'll be relying much more heavily on the data to come to a conclusion. \n",
    "\n",
    "### Maximum Likelihood Estimate (MLE)\n",
    "The most common way of estimating parameters in a parametric model is through the **Maximum Likelihood Estimate (MLE)**. The likelihood function is related to probability and is a function of the parameters of the model. To make this clear, think about the likelihood function as taking all of our data and saying, \"What is the most likely value for the mean and the most likely value for the standard deviation given the sample data that we see? \" So for the population, what is the most likely parameters given our sample? That's going to be our maximum likelihood estimate. Then we choose the value of each of those parameters that's going to maximize that likelihood function, that's going to maximize what is most likely to occur given the data that we have. \n",
    "\n",
    "### Machine Learning and Statistical Inference\n",
    "* Machine learning and Statistical inference are very similar. In both machine learning and statistical inference, we're using some sample data in order to infer qualities of the actual underlying population distribution in the real world, and the models that would have generated that data. The data-generating process can be thought of as a linear model representing the actual joint distribution between our x and the y variable. We may care either about the entire distribution when doing machine learning, or just some features of our distribution, such as just getting the point estimate of our mean, for example. \n",
    "* **Machine learning** that focuses on understanding the underlying parameters and individual effects of all of our parameters require tools pulled from **statistical inference**. On the other hand, some machine learning models tend to only mildly focus on all of these different parameters of the underlying parameters of our distribution, and just focus instead on prediction results, or just those estimates. \n",
    "\n",
    "#### Difference\n",
    "* First, statistical models generally try to explain the world, in terms of relation based on causality. On the contrary, machine learning only tries to mimic the work rather than explain it. \n",
    "* The statistical modern approach has a very specified model of the world that just needs to be estimated. Relation between observables are deduced in this approach. Different relations between observables are induced in the machine learning approach. This is because it does not have any a priori pre-specified model of the world and instead focuses on the predictive power. \n",
    "* Furthermore, statistical models typically deal with small data, with up to hundreds of attributes and up to thousands of examples, typically. On the other side, machine learning sometimes deal with data that might have the number of attributes in hundreds of thousands. And the number of examples in hundreds of millions because of these differences. \n",
    "* While scalability is normally not a major concern for statistical model and approaches. It sometimes becomes critical in machine learning applications. \n",
    "* Finally, statistical modeling is based on probabilistic approach. Some machine learning methods including support vector machines, neural nets, and some clustering methods are non-probabilistic. \n",
    "\n",
    "![ml_vs_stats.png](imgs/ml_vs_stats.png)\n",
    "\n",
    "#### Example\n",
    "Consider a business example of looking at customer churn. Customer churn occurs when a customer leaves a company, thus we need a lower churn rate. Data related to churn will include a target variable for whether or not a customer has left the company. Also, we include other features to help us make that prediction about whether a future customer leave, such as the length of time as a customer, the type and amount of purchases that customer has made, and other customer characteristics such as age, location, etc. Churn prediction is often approached by predicting a score for individuals that estimates the probability the customer will leave or not leave. So 0.99 means they're very likely to leave, 0.01 means we're probably going to hold on to that customer. \n",
    "\n",
    "In this example, we can estimate the impact of each feature. Think for every additional year someone has been a customer, that being the feature, they are 20 percent less likely to churn, so giving an estimate for the value of each additional year. For inference, we can compute the confidence interval for determining the statistical significance of the estimate. We can have 95 percent confidence interval on that estimate saying that the effect is either between 19 percent and 21 percent, meaning 20 percent is a good estimate of how much less likely they are to churn. On the other hand, the 95 percent confidence interval can be between negative 10 and 50 percent, meaning each additional year we're very uncertain if it's 20 percent, as a point estimate. For all we know we can actually have a negative effect or we can have a much stronger positive effects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Regression** is the most commonly used machine learning technique. Prediction of stock returns, markets, portfolio, credit losses, and many other problems amount to different sorts of regression. The other major class of supervised learning algorithms, **classification**, is also widely used. For example, it's used for such tasks as loan default model and credit rating predictions, credit card fraud, and anti-money laundering. In unsupervised learning, **clustering** our items sound more or less obvious. For example, such problems say segmentation of stocks, credit card holders, or institutional clients are all classical cases for clustering methods. Representation learning methods such as factor models or **Principal Component Analysis** are machine learning models. Other examples of machine learning approach are to regime change detections and methods to imputation of missing data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
