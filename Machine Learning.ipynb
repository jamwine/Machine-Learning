{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI vs ML vs DL\n",
    "### Artificial Intelligence (AI)\n",
    "A branch of computer science dealing with the simulation of intelligent behavior in computers. In layman's term, **AI** is a decision making algorithm that mimics human mind by doing smart work intelligently to solve complex problems.\n",
    "\n",
    "![ai](imgs/ai.png)\n",
    "\n",
    "### Machine Learning (ML)\n",
    "\n",
    "**Machine Learning** is the field of study that gives computers the ability to learn without being explicitly programmed. It is the study and construction of programs that are not explicitly programmed, but learn patterns as they are exposed to more data over time. The goal is to self-learn from data on certain task to maximize the performance of machine using experience on this task.\n",
    "\n",
    "A computer program is said to learn from **experience E** with respect to some class of **tasks T** and **performance measure P**, if its performance at tasks in T, as measured by P, improves with experience E. Consider an example, playing checkers where:\n",
    "* **E** = the experience of playing many games of checkers\n",
    "* **T** = the task of playing checkers.\n",
    "* **P** = the probability that the program will win the next game.\n",
    "\n",
    "In **supervised learning**, we are given a data set and already know what our correct output should look like, having the idea that there is a relationship between the input and the output. Supervised learning problems are categorized into *regression* and *classification* problems.\n",
    "\n",
    "**Unsupervised learning** allows us to approach problems with little or no idea what our results should look like. We can derive structure from data where we don't necessarily know the effect of the variables. We can derive this structure by *clustering* the data based on relationships among the variables in the data. With unsupervised learning there is no feedback based on the prediction results.\n",
    "\n",
    "### Deep Learning (DL)\n",
    "Deep learning is a subset of machine learning in which multilayered neural networks learn from vast amounts of data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differentiating Artificial Learning from Machine Learning\n",
    "|AI|ML|\n",
    "|---|---|\n",
    "|The aim is to increase chance of success and not accuracy.|The aim is to increase accuracy, but it does not care about success.|\n",
    "|AI will go for finding the optimal solution.|ML will go for only solution for that whether it is optimal or not.|\n",
    "|AI leads to intelligence or wisdom.|ML leads to knowledge.|\n",
    "|AI manages more comprehensive issues of automating a system. This computerization should be possible by utilizing any field such as image processing, cognitive science, neural systems, machine learning etc.|Machine Learning (ML) manages influencing user’s machine to gain from the external environment. This external environment can be sensors, electronic segments, external storage gadgets and numerous other devices.|\n",
    "|AI manages the making of machines, frameworks and different gadgets savvy by enabling them to think and do errands as all people generally do.|What ML does, depends on the user input or a query requested by the client, the framework checks whether it is available in the knowledge base or not. If it is available, it will restore the outcome to the user related with that query, however if it isn’t stored initially, the machine will take in the user input and will enhance its knowledge base, to give a better value to the end user.|\n",
    "\n",
    "Examples of **Artificial Intelligence** include:\n",
    "* Voice assistants, such as Siri\n",
    "* Recommendation systems, such as Netflix\n",
    "* Self-driving cars\n",
    "* Drones that fly over fields and capture footage used to optimize crop yield\n",
    "* Google Search\n",
    "* Surfacing algorithms, such as those employed by Twitter and Facebook, that decide what content to show you in our feed\n",
    "\n",
    "Examples of **Machine Learning** include:\n",
    "* Predicting whether a given credit card transaction is fraudulent or not, given transaction details\n",
    "* Predicting whether an email is spam or not, given the email sender, subject, and body\n",
    "* Predicting the diagnosis of a particular piece of medical imaging\n",
    "* Predicting the present and future location of pedestrians, cars, and other stationary/moving objects in a video feed (such as those used by self-driving cars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning Workflow\n",
    "The machine learning workflow consists of:\n",
    "\n",
    "* **Problem statement**: What problem are we trying to solve?\n",
    "* **Data collection**: What data do you need to solve it?\n",
    "* **Data exploration and preprocessing**: How should we clean our data so our model can use it?\n",
    "* **Modeling**: Build a model to solve our problem?\n",
    "* **Validation**: Did we solve the problem?\n",
    "* **Decision Making and Deployment**: Communicate to stakeholders or put into production?\n",
    "\n",
    "### Common Terminologies of data for Machine Learning:\n",
    "* **target**: category or value we are trying to predict\n",
    "* **observation**: an example or single data point within the data (usually a point or row in dataset)\n",
    "* **features**: explanatory variables used for prediction for each observation (usually a column)\n",
    "* **label**: the value of the target for a single data point (output variable being predicted)\n",
    "* **algorithms**: computer programs that estimate models based on available data\n",
    "* **model**: hypothesized relationship between observations and data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming Data\n",
    "\n",
    "Models used in Machine Learning Workflows often make assumptions about the data. For example, linear regression model assumes a linear relationship between observations and target (outcome) variables. An example of a linear model relating (feature) variables $x_1$ and $x_2$ with target (label) variable $y$, is:\n",
    "\n",
    "$$y_\\beta(x)=\\beta_0+\\beta_1x_1+\\beta_2x_2$$\n",
    "\n",
    "where $\\beta=(\\beta_0,\\beta_1,\\beta_2)$ represents the model's parameter.\n",
    "\n",
    "Predictions from linear regression models assume **residuals are normally distributed**. However, the raw data (having features) and predicted data are often **skewed** (distorted away from the center), hence we TRANSFORM the data.\n",
    "\n",
    "**Log transformations** can be a useful way to find a linear relationship when the underlying raw data may not actually have a linear relationship. \n",
    "```python\n",
    "# Useful transformation functions\n",
    "from numpy import log,log1p\n",
    "from scipy.stats import boxcox\n",
    "```\n",
    "\n",
    "So the resulting algorithm will still be a linear regression since the outcome is still a linear combinations of the features $y_\\beta(x)=\\beta_0+\\beta_1log(x_1)$ (the features have been transformed). Now the linear regression involves a linear combination of our new features, one of our new features being the log(x) rather than just x.\n",
    "\n",
    "Similarly, we can estimate higher-order relationships by adding polynomial features to fit a linear model. \n",
    "$$y_\\beta(x)=\\beta_0+\\beta_1x+\\beta_2x^2$$\n",
    "\n",
    "Again, we're changing our features, but maintaining a linear model with features being transformed into squared and cubed.\n",
    "\n",
    "### Variable Selection\n",
    "This involves choosing the set of features to include in the model. Variables must often be transformed before they can be included in models. In addition to log and polynomial transformations, this can involve:\n",
    "* **Encoding**: converting non-numeric features to numeric features\n",
    "* **Scaling**: converting the scale of numeric data so they are comparable.\n",
    "\n",
    "The appropriate method of encoding and scaling depends on the type of feature. **Feature Encoding** is often applied to categorical features, two primary types are: \n",
    "* Nominal: categorical variables takes values in unordered categories (eg. red, blue, green, True, False)\n",
    "* Ordinal: categorical variables takes values in ordered categories (eg. low, medium, high)\n",
    "\n",
    "There are several common approaches to encoding variables:\n",
    "* **Binary Encoding**: converts variable to either 0 or 1 and is suitable for variables that takes two possible values (eg. True, False)\n",
    "* **One-hot Encoding**: converts variables that takes multiple values into binary (0,1) variables, one for each category. This creates several new variables.\n",
    "* **Ordinal Encoding**: involves converting ordered categories to numerical values, usually by creating one variable that takes integer equal to the number of categories (eg. 0,1,2,3,...).\n",
    "\n",
    "**Feature Scaling** involves adjusting a variable's scale. This allows comparison of variables with different scales, as different continuous (numeric) features often have different scales. Some common approaches to scaling features are:\n",
    "* **Standard Scaling**: converts features to standard normal variables (by subtracting the mean and dividing by the standard error)\n",
    "* **Min-max Scaling**: converts variables to continuous variables in the (0,1) interval by mapping minimum values to 0 and maximum values to 1. This type of scaling is sensitive to outliers. \n",
    "* **Robust Scaling**: is similar to min-max scaling, but instead maps the interquartile range to (0,1). This means the variable itself takes values outside of the (0,1) interval.\n",
    "\n",
    "```python\n",
    "# Common variable transformation\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "\n",
    "# Functions for encoding categorical variables\n",
    "from sklearn.preprocessing import LabelEncoder, LabelBinarizer, OneHotEncoder\n",
    "from pandas import get_dummies\n",
    "\n",
    "# Functions for encoding ordinal variables\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write a difference between statistical tools that are used for prediction and machine learning tools that are used for classification. Examples are required.\n",
    "\n",
    "* First, statistical models generally try to explain the world, in terms of relation based on causality. On the contrary, machine learning only tries to mimic the work rather than explain it. \n",
    "* The statistical modern approach has a very specified model of the world that just needs to be estimated. Relation between observables are deduced in this approach. Different relations between observables are induced in the machine learning approach. This is because it does not have any a priori pre-specified model of the world and instead focuses on the predictive power. \n",
    "* Furthermore, statistical models typically deal with small data, with up to hundreds of attributes and up to thousands of examples, typically. On the other side, machine learning sometimes deal with data that might have the number of attributes in hundreds of thousands. And the number of examples in hundreds of millions because of these differences. \n",
    "* While scalability is normally not a major concern for statistical model and approaches. It sometimes becomes critical in machine learning applications. \n",
    "* Finally, statistical modeling is based on probabilistic approach. Some machine learning methods including support vector machines, neural nets, and some clustering methods are non-probabilistic. \n",
    "\n",
    "![ml_vs_stats.png](imgs/ml_vs_stats.png)\n",
    "\n",
    "**Regression** is the most commonly used machine learning technique. Prediction of stock returns, markets, portfolio, credit losses, and many other problems amount to different sorts of regression. The other major class of supervised learning algorithms, **classification**, is also widely used. For example, it's used for such tasks as loan default model and credit rating predictions, credit card fraud, and anti-money laundering. In unsupervised learning, **clustering** our items sound more or less obvious. For example, such problems say segmentation of stocks, credit card holders, or institutional clients are all classical cases for clustering methods. Representation learning methods such as factor models or **Principal Component Analysis** are machine learning models. Other examples of machine learning approach are to regime change detections and methods to imputation of missing data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
